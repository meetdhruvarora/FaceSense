# FaceSense

**FaceSense** is a real-time emotion detection and video analysis system that combines computer vision, deep learning, and speech recognition. It performs facial expression classification, eye-tracking, and audio transcription to detect emotions and context-based behavior ‚Äî ideal for surveillance, remote interviews, and academic integrity monitoring.

---

## ‚úÖ Features

- **Real-Time Emotion Detection**  
  Classifies emotions such as happy, sad, angry, neutral, etc., with ~85% accuracy using a CNN model (TensorFlow/Keras).

- **Facial and Eye Tracking**  
  Detects facial landmarks and tracks eye movement across over 10,000 frames per session using OpenCV and dlib.

- **Speech-to-Text Analysis**  
  Uses Python's `SpeechRecognition` and `PyAudio` libraries to transcribe speech in real time for audio context.

- **Integrated Multi-Modal Analysis**  
  Combines visual (face, eyes) and auditory (speech) signals to strengthen emotion detection and identify suspicious or anomalous behavior.

---

## Screenshots

- ![image](https://github.com/user-attachments/assets/550973d9-bc4f-4510-81a3-0ccca8be251b)


## üõ†Ô∏è Technologies & Dependencies

This project is built using:

- **Language**: Python 3.8+
- **Libraries**:
  - `opencv-python`
  - `dlib`
  - `tensorflow`
  - `keras`
  - `SpeechRecognition`
  - `pyaudio`
  - `numpy`
  - `pandas`
